{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leZnIISQ3pQC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Input\n",
        "from keras.layers.merge import add\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "import random\n",
        "random.seed(10)\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsZ7CNIs2R2D"
      },
      "source": [
        "Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVFOxzxs3pY7"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"trainingSet.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "data.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxez54nq32tM"
      },
      "outputs": [],
      "source": [
        "tags = list(set(data[\"TAG\"].values))\n",
        "tagCount = len(tags)\n",
        "tagCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whhPR3o3m2KE"
      },
      "outputs": [],
      "source": [
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD-0KBaA32xI"
      },
      "outputs": [],
      "source": [
        "class getSentence(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"TAG\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"sent_id\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"{}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzhm5f1y320m"
      },
      "outputs": [],
      "source": [
        "getSen = getSentence(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riYSo-G64Bmp"
      },
      "outputs": [],
      "source": [
        "sentences = getSen.sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbJWHsNK4Bp5"
      },
      "outputs": [],
      "source": [
        "max_len = 50\n",
        "tag2id = {t: i for i, t in enumerate(tags)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt5ssSr-4ITz"
      },
      "outputs": [],
      "source": [
        "X_tr = [[w[0] for w in s] for s in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHgLNwkj4IXe"
      },
      "outputs": [],
      "source": [
        "new_X = []\n",
        "for seq in X_tr:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"__PAD__\")\n",
        "    new_X.append(new_seq)\n",
        "X_tr = new_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9gKAb8W4JK9"
      },
      "outputs": [],
      "source": [
        "y_tr = [[tag2id[w[1]] for w in s] for s in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_s6BRfm4JOY"
      },
      "outputs": [],
      "source": [
        "y_tr = pad_sequences(maxlen=max_len, sequences=y_tr, padding=\"post\", value=tag2id[\"O\"], truncating = 'post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmFEjd_g2K5g"
      },
      "source": [
        "Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYt3NlAM2GbV"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"validationSet.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "data.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHnwGmvS2haV"
      },
      "outputs": [],
      "source": [
        "getSen = getSentence(data)\n",
        "sentences = getSen.sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDjiVIpo27Ok"
      },
      "outputs": [],
      "source": [
        "X_te = [[w[0] for w in s] for s in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCZ3uBpD27R4"
      },
      "outputs": [],
      "source": [
        "new_X = []\n",
        "for seq in X_te:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"__PAD__\")\n",
        "    new_X.append(new_seq)\n",
        "X_te = new_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOkgqrWY27YS"
      },
      "outputs": [],
      "source": [
        "y_te = [[tag2id[w[1]] for w in s] for s in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwmVwL1-27bW"
      },
      "outputs": [],
      "source": [
        "y_te = pad_sequences(maxlen=max_len, sequences=y_te, padding=\"post\", value=tag2id[\"O\"], truncating = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dehbXJwJ4iN3"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cJZkt-t4iUH"
      },
      "outputs": [],
      "source": [
        "sess = tf.Session()\n",
        "K.set_session(sess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_BjWXf24VCM"
      },
      "source": [
        "Elmo Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAv3byjk4iXB"
      },
      "outputs": [],
      "source": [
        "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBlMbpOu4ibB"
      },
      "outputs": [],
      "source": [
        "def ElmoEmbedding(x):\n",
        "    return elmo_model(inputs={\n",
        "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
        "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
        "                      },\n",
        "                      signature=\"tokens\",\n",
        "                      as_dict=True)[\"elmo\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6al8dopN4ZhN"
      },
      "source": [
        "Building deep learning neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rrHEHxh4wy5"
      },
      "outputs": [],
      "source": [
        "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
        "embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_text)\n",
        "x = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
        "x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n",
        "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
        "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
        "out = TimeDistributed(Dense(tagCount, activation=\"softmax\"))(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1IKkPd84w2J"
      },
      "outputs": [],
      "source": [
        "model = Model(input_text, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx-utQVp4w5E"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "lVVJwx3e0s61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn8sajrZ4w8P"
      },
      "outputs": [],
      "source": [
        "X_tr, X_val = X_tr[:944*batch_size], X_te[-103*batch_size:]\n",
        "y_tr, y_val = y_tr[:944*batch_size], y_te[-103*batch_size:]\n",
        "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
        "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F93wY09A4_mh"
      },
      "outputs": [],
      "source": [
        "history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),\n",
        "                    batch_size=batch_size, epochs=5, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_fF3S9d4_qB"
      },
      "outputs": [],
      "source": [
        "hist = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHZUvsex5IBo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])\n",
        "plt.title(\"Learning curves\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Set"
      ],
      "metadata": {
        "id": "9FEF5umJZrI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdMBClxugUzS"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"testSet.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\")\n",
        "data.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCO1xaJ5gUzS"
      },
      "outputs": [],
      "source": [
        "getSen = getSentence(data)\n",
        "sentences = getSen.sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grm-T3fogUzT"
      },
      "outputs": [],
      "source": [
        "X_test = [[w[0] for w in s] for s in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_JuTk3FgUzT"
      },
      "outputs": [],
      "source": [
        "new_X = []\n",
        "for seq in X_test:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"__PAD__\")\n",
        "    new_X.append(new_seq)\n",
        "X_test = new_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8towKMEgUzT"
      },
      "outputs": [],
      "source": [
        "y_test = [[tag2id[w[1]] for w in s] for s in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmGLDrgtgUzU"
      },
      "outputs": [],
      "source": [
        "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2id[\"O\"], truncating = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTgWCxIygUzV"
      },
      "outputs": [],
      "source": [
        "fileToWrite = open(\"test_output_wSentid\",\"a\")\n",
        "fileToWrite.write(\"{:6}{:30}@#armo{:15}@#armo({})\".format(\"sent_id\",\"Word\", \"True\", \"Pred\"))\n",
        "fileToWrite.write(\"\\n\")\n",
        "fileToWrite.write(\"=\"*30)\n",
        "fileToWrite.write(\"\\n\")\n",
        "\n",
        "i=0\n",
        "print(\"{:6}{:30}@#armo{:15}@#armo({})\".format(\"sent_id\",\"Word\", \"True\", \"Pred\"))\n",
        "print(\"=\"*30)\n",
        "while i<3022:\n",
        "  p = model.predict(np.array(X_test[i:i+batch_size]))[0]\n",
        "  p = np.argmax(p, axis=-1)\n",
        "  for w, true, pred in zip(X_test[i], y_test[i], p):\n",
        "    if w != \"__PAD__\":\n",
        "      # print(\"{:20}:{:8} ({})\".format(w, tags[true], tags[pred]))\n",
        "      fileToWrite.write(\"{:6} {:30}@#armo{:15}@#armo({})\".format(i+1, w, tags[true], tags[pred]))\n",
        "      fileToWrite.write(\"\\n\")\n",
        "  fileToWrite.write(\"=\"*30)\n",
        "  fileToWrite.write(\"\\n\") \n",
        "  i = i+1\n",
        "fileToWrite.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr-pJTgJgUzV"
      },
      "outputs": [],
      "source": [
        "!head -n 30 test_output_wSentid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIYVciibgUzV"
      },
      "outputs": [],
      "source": [
        "fileRead = open(\"test_output_wSentid\", \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMNyFTgpgUzW"
      },
      "outputs": [],
      "source": [
        "f2 = fileRead.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oOay3lXgUzW"
      },
      "outputs": [],
      "source": [
        "len(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2psls_ibgUzX"
      },
      "outputs": [],
      "source": [
        "#Actually source, cue or content and predicted as the same\n",
        "tp = 0\n",
        "#Actually O but, predicted as source, cue or content\n",
        "fp = 0\n",
        "#Actually O and predicted as the same\n",
        "tn = 0\n",
        "#Actually source, cue or content but, predicted as O\n",
        "fn = 0\n",
        "tpAR = 0\n",
        "tpARList = []\n",
        "total = len(f2)\n",
        "current_fp_fn =0\n",
        "exact_sent_cnt =-1\n",
        "senCount = -1\n",
        "arSenIds = []\n",
        "allZero = True\n",
        "allZeroSenID = []\n",
        "attriSenID = []\n",
        "allZeroMatched = -1\n",
        "total = len(f2)\n",
        "for index in range(1, total):\n",
        "    i = f2[index]\n",
        "    if (i == \"==============================\\n\"):\n",
        "        senCount += 1\n",
        "        new_fp_fn_cnt = fp + fn\n",
        "        if (new_fp_fn_cnt == current_fp_fn):\n",
        "            if allZero == True:\n",
        "                allZeroMatched += 1\n",
        "                if senCount>0:\n",
        "                    allZeroSenID.append(senCount)  \n",
        "            else:\n",
        "                allZero = True\n",
        "                if senCount>0:\n",
        "                    attriSenID.append(senCount)\n",
        "            exact_sent_cnt+=1\n",
        "            if senCount>0:\n",
        "                arSenIds.append(senCount)\n",
        "        current_fp_fn = new_fp_fn_cnt\n",
        "        continue\n",
        "    tok = i.split('@#armo')\n",
        "    try:\n",
        "        actual = tok[1].strip()\n",
        "        predicted = tok[2][1:-2].strip()\n",
        "        if actual == 'O':\n",
        "            if actual == predicted:\n",
        "                tn += 1\n",
        "#                 tnList.append(i)\n",
        "            else:\n",
        "                fp += 1\n",
        "#                 fpList.append(i)\n",
        "        else:\n",
        "            allZero = False\n",
        "            if actual == predicted:\n",
        "                tp += 1\n",
        "#                 tpList.append(i)\n",
        "            else:\n",
        "                fn += 1\n",
        "#                 fnList.append(i)\n",
        "    except IndexError:\n",
        "        print(tok)\n",
        "        break   \n",
        "print(\"Total sentences= \"+str(senCount))\n",
        "print(\"Exact words match sentences =\"+str(exact_sent_cnt))\n",
        "print(\"Exact matching sentences Id list\")\n",
        "print(arSenIds[0:20])\n",
        "print (\"Number of sentences with all O = \"+str(allZeroMatched))\n",
        "print(\"All O sentence IDs\")\n",
        "print(allZeroSenID[0:10])\n",
        "print(\"Attribution relations sentence IDs\")\n",
        "print(attriSenID[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating confusion matrix\n",
        "import numpy as np\n",
        "conf = np.zeros((4,4))\n",
        "total = len(f2)\n",
        "for index in range(1, total):\n",
        "    i = f2[index]\n",
        "    if (i == \"==============================\\n\"):\n",
        "        continue\n",
        "    tok = i.split('@#armo')\n",
        "    try:\n",
        "        actual = tok[1].strip()\n",
        "        predicted = tok[2][1:-2].strip()\n",
        "        \n",
        "        if actual == 'source':\n",
        "            if actual == predicted:\n",
        "                conf[0][0] += 1\n",
        "            elif predicted == 'cue':\n",
        "                conf[1][0] += 1\n",
        "            elif predicted == 'content':\n",
        "                conf[2][0] += 1\n",
        "            else:\n",
        "                conf[3][0] += 1\n",
        "        if actual == 'cue':\n",
        "            if actual == predicted:\n",
        "                conf[1][1] += 1\n",
        "            elif predicted == 'source':\n",
        "                conf[0][1] += 1\n",
        "            elif predicted == 'content':\n",
        "                conf[2][1] += 1\n",
        "            else:\n",
        "                conf[3][1] += 1\n",
        "        if actual == 'content':\n",
        "            if actual == predicted:\n",
        "                conf[2][2] += 1\n",
        "            elif predicted == 'source':\n",
        "                conf[0][2] += 1\n",
        "            elif predicted == 'cue':\n",
        "                conf[1][2] += 1\n",
        "            else:\n",
        "                conf[3][2] += 1\n",
        "        if actual == 'O':\n",
        "            if actual == predicted:\n",
        "                conf[3][3] += 1\n",
        "            elif predicted == 'source':\n",
        "                conf[0][3] += 1\n",
        "            elif predicted == 'cue':\n",
        "                conf[1][3] += 1\n",
        "            else:\n",
        "                conf[2][3] += 1\n",
        "\n",
        "    except IndexError:\n",
        "        print(tok)\n",
        "        break \n",
        "print(conf) "
      ],
      "metadata": {
        "id": "qL1bJ9aSk0Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token-wise performance\n",
        "y_true = []\n",
        "y_pred = []\n",
        "total = len(f2)\n",
        "for index in range(1, total):\n",
        "    i = f2[index]\n",
        "    if (i == \"==============================\\n\"):\n",
        "        continue\n",
        "    tok = i.split('@#armo')\n",
        "    actual = tok[1].strip()\n",
        "    predicted = tok[2][1:-2].strip() \n",
        "    y_true.append(actual)\n",
        "    y_pred.append(predicted)\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "m_upOByOLVMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8JAafb8TaYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}